import os
import spacy as sp
from transformers import pipeline
from pathlib import Path
from mailcom.inout import InoutHandler
from mailcom.utils import check_dir, make_dir, LangDetector

# please modify this section depending on your setup
# input language - either "es" or "fr"
# will also need pt
lang = "es"
# lang = "fr"
# path where the input files can be found
path_input = Path("./mailcom/test/data/")
# path where the output files should be written to
# this is generated if not present yet
path_output = Path("./data/out/")
output_filename = "dict"
# the ner tool - currently only "transformers"
tool = "transformers"
# please do not modify below this section unless you know what you are doing


class Pseudonymize:
    def __init__(self):

        self.spacy_default_model_dict = {
            "es": "es_core_news_md",
            "fr": "fr_core_news_md",
        }

        self.pseudo_first_names = {
            "es": [
                "José",
                "Angel",
                "Alex",
                "Ariel",
                "Cruz",
                "Fran",
                "Arlo",
                "Adri",
                "Marce",
                "Mati",
            ],
            "fr": [
                "Claude",
                "Dominique",
                "Claude",
                "Camille",
                "Charlie",
                "Florence",
                "Francis",
                "Maxime",
                "Remy",
                "Cécile",
            ],
        }

        # records NEs in the last email
        self.ne_list = []

    def init_spacy(self, language: str, model="default"):
        if model == "default":
            model = self.spacy_default_model_dict[language]
        try:
            # disable not needed components
            self.nlp_spacy = sp.load(
                model, exclude=["morphologizer", "attribute_ruler", "lemmatizer", "ner"]
            )
        except OSError:
            try:
                print(
                    "Could not find model in standard directory. Trying to download model from repo."  # noqa
                )
                # try downloading model
                sp.cli.download(model)
                self.nlp_spacy = sp.load(
                    model,
                    exclude=["morphologizer", "attribute_ruler", "lemmatizer", "ner"],
                )
            except SystemExit:
                raise SystemExit("Could not download {} from repo".format(model))

    def init_transformers(
        self,
        model="xlm-roberta-large-finetuned-conll03-english",
        model_revision_number="18f95e9",
    ):
        self.ner_recognizer = pipeline(
            "token-classification",
            model=model,
            revision=model_revision_number,
            aggregation_strategy="simple",
        )

    def reset(self):
        # reset NEs
        self.ne_list.clear()

    def get_sentences(self, input_text):
        doc = self.nlp_spacy(input_text)
        text_as_sents = []
        for sent in doc.sents:
            text_as_sents.append(str(sent))
        return text_as_sents

    def get_ner(self, sentence):
        ner = self.ner_recognizer(sentence)
        return ner

    def choose_per_pseudonym(self, name):
        pseudonym = ""
        # get list of already replaced names, and list of corresponding pseudonyms
        used_names = [ne["word"] for ne in self.ne_list]
        used_pseudonyms = [
            ne["pseudonym"] if "pseudonym" in ne else "" for ne in self.ne_list
        ]
        # amount of pseudonyms for PER used (PER for "PERSON")
        n_pseudonyms_used = [ne["entity_group"] for ne in self.ne_list].count("PER")
        # check all variations of the name
        name_variations = [
            name,
            name.lower(),
            name.title(),
        ]
        # if this name has been replaced before, choose the same pseudonym
        for nm_var in name_variations:
            pseudonym = (
                used_pseudonyms[used_names.index(nm_var)]
                if nm_var in used_names
                else ""
            )
            if pseudonym != "":
                break
            # if none is found, choose a new pseudonym
            if pseudonym == "":
                try:
                    # TODO: clean up the below comment after addressing the question
                    # question: is the new pseudonym always a France one?
                    pseudonym = self.pseudo_first_names["fr"][
                        n_pseudonyms_used
                    ]  # reaches end of the list
                except IndexError:
                    pseudonym = self.pseudo_first_names["fr"][0]
        return pseudonym

    def pseudonymize_ne(self, ner, sentence):
        # remove any named entities
        new_sentence = sentence
        # record offset generated by pseudonym lengths different than NE lengths
        offset = 0
        for _, entity in enumerate(ner):
            # process NE
            ent_string = entity["entity_group"]
            ent_word = entity["word"]
            start, end = entity["start"], entity["end"]
            # choose the pseudonym of current NE based on its type
            if ent_string == "PER":
                pseudonym = self.choose_per_pseudonym(ent_word)
            # replace LOC
            elif ent_string == "LOC":
                pseudonym = "[location]"
            # replace ORG
            elif ent_string == "ORG":
                pseudonym = "[organization]"
            # replace MISC
            elif ent_string == "MISC":
                pseudonym = "[misc]"

            # add the pseudonym to the entity dict
            entity["pseudonym"] = pseudonym

            # add this entity to the total NE list
            self.ne_list.append(entity)

            # replace the NE with its pseudonym
            # only replace this occurence of the NE by using start and end positions
            new_sentence = (
                new_sentence[: start + offset]
                + pseudonym
                + new_sentence[end + offset :]  # noqa
            )
            # update offset
            offset += len(pseudonym) - len(ent_word)

        # return new sentence
        newlist = [new_sentence]
        return newlist

    def pseudonymize_numbers(self, sentence):
        sent_as_list = list(sentence)
        new_list = []
        for i in range(len(sent_as_list)):
            if sent_as_list[i].isdigit():
                if i == 0 or not sent_as_list[i - 1].isdigit():
                    new_list.append("[number]")
            else:
                new_list.append(sent_as_list[i])

        return "".join(new_list)

    def pseudonymize_email_addresses(self, sentence):
        split = sentence.split(" ")
        new_list = []
        for word in split:
            if "@" in word:
                new_list.append("[email]")
            else:
                new_list.append(word)
        return " ".join(new_list)

    def concatenate(self, sentences):
        return " ".join(sentences)

    def pseudonymize(self, email):
        """Function that handles the pseudonymization of an email
        and all its steps

        Args:
            email (dict): Dictionary containing email content and metadata.

        Returns:
            str: Pseudonymized text"""
        text = email["content"]
        self.reset()
        sentences = self.get_sentences(text)
        pseudonymized_sentences = []
        for sent in sentences:
            sent = self.pseudonymize_email_addresses(sent)
            ner = self.get_ner(sent)
            ps_sent = " ".join(self.pseudonymize_ne(ner, sent)) if ner else sent
            ps_sent = self.pseudonymize_numbers(ps_sent)
            pseudonymized_sentences.append(ps_sent)
        email["pseudo_content"] = self.concatenate(pseudonymized_sentences)
        return email["pseudo_content"]


if __name__ == "__main__":
    # nlp_spacy = init_spacy(lang)
    # nlp_transformers = init_transformers()

    # check that input dir is there
    if not check_dir(path_input):
        raise ValueError("Could not find input directory with eml files! Aborting ...")

    # check that the output dir is there, if not generate
    if not check_dir(path_output):
        print("Generating output directory/ies.")
        make_dir(path_output)
    # process the text
    io = InoutHandler(path_input)
    io.list_of_files()
    io.process_emails()
    # html_files = list_of_files(path_input, "html")
    pseudonymizer = Pseudonymize()
    pseudonymizer.init_spacy("fr")
    pseudonymizer.init_transformers()
    for idx, email in enumerate(io.get_email_list()):
        if not email["content"]:
            continue
        # Test functionality of Pseudonymize class
        _ = pseudonymizer.pseudonymize(email)
        print("New text:", email["pseudo_content"])
        print("Old text:", email["content"])
    io.write_csv("data/out/out.csv")
